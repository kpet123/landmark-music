{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory body landmark in oboe playign\n",
    "\n",
    "Author: Kaitlin Pet \n",
    "2023\n",
    "\n",
    "\n",
    "- Compare mutliple takes of oboe performance. Want to see if landmarks are similar in both takes \n",
    "- video orchestra folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RT demo from https://github.com/google/mediapipe/blob/master/docs/solutions/holistic.md\n",
    "- stopping this will make kernel die! perform with care. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_drawing_styles = mp.solutions.drawing_styles\n",
    "mp_holistic = mp.solutions.holistic\n",
    "\n",
    "\n",
    "# For webcam input:\n",
    "cap = cv2.VideoCapture(0)\n",
    "with mp_holistic.Holistic(\n",
    "    min_detection_confidence=0.5,\n",
    "    min_tracking_confidence=0.5) as holistic:\n",
    "  while cap.isOpened():\n",
    "    success, image = cap.read()\n",
    "    if not success:\n",
    "      print(\"Ignoring empty camera frame.\")\n",
    "      # If loading a video, use 'break' instead of 'continue'.\n",
    "      continue\n",
    "\n",
    "    # To improve performance, optionally mark the image as not writeable to\n",
    "    # pass by reference.\n",
    "    image.flags.writeable = False\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    results = holistic.process(image)\n",
    "    print(\"image detected\")\n",
    "    # Draw landmark annotation on the image.\n",
    "    image.flags.writeable = True\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "    mp_drawing.draw_landmarks(\n",
    "        image,\n",
    "        results.face_landmarks,\n",
    "        mp_holistic.FACEMESH_CONTOURS,\n",
    "        landmark_drawing_spec=None,\n",
    "        connection_drawing_spec=mp_drawing_styles\n",
    "        .get_default_face_mesh_contours_style())\n",
    "    mp_drawing.draw_landmarks(\n",
    "        image,\n",
    "        results.pose_landmarks,\n",
    "        mp_holistic.POSE_CONNECTIONS,\n",
    "        landmark_drawing_spec=mp_drawing_styles\n",
    "        .get_default_pose_landmarks_style())\n",
    "    \n",
    "    mp_drawing.draw_landmarks(\n",
    "        image,\n",
    "        results.left_hand_landmarks,\n",
    "        mp_holistic.HAND_CONNECTIONS,\n",
    "        landmark_drawing_spec=mp_drawing_styles\n",
    "        .get_default_hand_landmarks_style())\n",
    "\n",
    "    mp_drawing.draw_landmarks(\n",
    "        image,\n",
    "        results.right_hand_landmarks,\n",
    "        mp_holistic.HAND_CONNECTIONS,\n",
    "        landmark_drawing_spec=mp_drawing_styles\n",
    "        .get_default_hand_landmarks_style())\n",
    "    # Flip the image horizontally for a selfie-view display.\n",
    "    cv2.imshow('MediaPipe Holistic', cv2.flip(image, 1))\n",
    "    if cv2.waitKey(5) & 0xFF == 27:\n",
    "      print(\"detection not working in jupyter\")\n",
    "      break\n",
    "cap.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": "true",
    "tags": []
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uncomment requirements to install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install librosa\n",
    "#!pip install libfmp\n",
    "!pip3 install mediapipe\n",
    "!pip install opencv-python\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#indicates experiments being done on Kaitlin's computer vs server. Will consolidate version with nessesary files\n",
    "local= True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import mediapipe as mp\n",
    "from mediapipe.tasks import python\n",
    "from mediapipe.tasks.python import vision\n",
    "#import librosa\n",
    "import cv2\n",
    "#from google.colab.patches import cv2_imshow\n",
    "import os\n",
    "import pickle\n",
    "BaseOptions = mp.tasks.BaseOptions\n",
    "HandLandmarker = mp.tasks.vision.HandLandmarker\n",
    "HandLandmarkerOptions = mp.tasks.vision.HandLandmarkerOptions\n",
    "VisionRunningMode = mp.tasks.vision.RunningMode\n",
    "import os\n",
    "import pandas as pd\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for demo\n",
    "\n",
    "view = 'front'\n",
    "take = '1'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": "true"
   },
   "source": [
    "# Get alignment from orchestra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "perf_atm = os.path.join(\"./videos\", view, view+\"_take\"+take, view+\"_take\"+take+'.atm')\n",
    "perf_atm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_alignment_from_orchestra(perf_atm, vid_frame_rate = 60):\n",
    "    df = pd.read_csv(perf_atm, sep = r'\\s{1,}', names= ['beat', 'time'])\n",
    "    df = df.loc[df['time']!= 0]\n",
    "    df['vidframe']= df['time']*vid_frame_rate\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "note_position_df = get_alignment_from_orchestra(perf_atm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": "true",
    "tags": []
   },
   "source": [
    "# Hand detection alone#download off-the-shelf model\n",
    "\n",
    "!wget -q https://storage.googleapis.com/mediapipe-models/hand_landmarker/hand_landmarker/float16/1/hand_landmarker.task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization function: shows how to yield values from fitted model\n",
    "from mediapipe import solutions\n",
    "from mediapipe.framework.formats import landmark_pb2\n",
    "import numpy as np\n",
    "\n",
    "MARGIN = 10  # pixels\n",
    "FONT_SIZE = 1\n",
    "FONT_THICKNESS = 1\n",
    "HANDEDNESS_TEXT_COLOR = (88, 205, 54) # vibrant green\n",
    "\n",
    "def draw_landmarks_on_image(rgb_image, detection_result):\n",
    "  hand_landmarks_list = detection_result.hand_landmarks\n",
    "  handedness_list = detection_result.handedness\n",
    "  annotated_image = np.copy(rgb_image)\n",
    "\n",
    "  # Loop through the detected hands to visualize.\n",
    "\n",
    "  #print(\"length of identified landmarks: \", len(hand_landmarks_list))\n",
    "  for idx in range(len(hand_landmarks_list)):\n",
    "\n",
    "    #print(\"*****\\n idx is \", idx)\n",
    "    hand_landmarks = hand_landmarks_list[idx]\n",
    "    handedness = handedness_list[idx]\n",
    "    #print( hand_landmarks)\n",
    "\n",
    "    # Draw the hand landmarks.\n",
    "    hand_landmarks_proto = landmark_pb2.NormalizedLandmarkList()\n",
    "    hand_landmarks_proto.landmark.extend([\n",
    "      landmark_pb2.NormalizedLandmark(x=landmark.x, y=landmark.y, z=landmark.z) for landmark in hand_landmarks\n",
    "    ])\n",
    "    solutions.drawing_utils.draw_landmarks(\n",
    "      annotated_image,\n",
    "      hand_landmarks_proto,\n",
    "      solutions.hands.HAND_CONNECTIONS,\n",
    "      solutions.drawing_styles.get_default_hand_landmarks_style(),\n",
    "      solutions.drawing_styles.get_default_hand_connections_style())\n",
    "\n",
    "    # Get the top left corner of the detected hand's bounding box.\n",
    "    height, width, _ = annotated_image.shape\n",
    "    x_coordinates = [landmark.x for landmark in hand_landmarks]\n",
    "    y_coordinates = [landmark.y for landmark in hand_landmarks]\n",
    "    text_x = int(min(x_coordinates) * width)\n",
    "    text_y = int(min(y_coordinates) * height) - MARGIN\n",
    "\n",
    "    # Draw handedness (left or right hand) on the image.\n",
    "    cv2.putText(annotated_image, f\"{handedness[0].category_name}\",\n",
    "                (text_x, text_y), cv2.FONT_HERSHEY_DUPLEX,\n",
    "                FONT_SIZE, HANDEDNESS_TEXT_COLOR, FONT_THICKNESS, cv2.LINE_AA)\n",
    "\n",
    "  return annotated_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hand_landmarks(src_color, detector, detector_type = 'video', timestamp=None, display=False):\n",
    "\n",
    "\n",
    "    #make sure color space is correct before converting to mediapipe readable image type\n",
    "    image= cv2.cvtColor(src_color, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    if detector_type =='image':\n",
    "        mp_image = mp.Image(image_format=mp.ImageFormat.SRGB, data=image)\n",
    "\n",
    "        # STEP 4: Detect hand landmarks from the input image.\n",
    "        detection_result = detector.detect(mp_image)\n",
    "    elif detector_type == 'video':\n",
    "\n",
    "        mp_image = mp.Image(image_format=mp.ImageFormat.SRGB, data=image)\n",
    "        detection_result = detector.detect_for_video(mp_image, timestamp)\n",
    "    else:\n",
    "        print(\"invalid detector type, should be \\n\\\"image\\\"\\n or \\n\\\"video\\\"\\n\")\n",
    "    # Extract normalized hand landmarks\n",
    "    hand_landmarks_list = detection_result.hand_landmarks\n",
    "    handedness_list = detection_result.handedness\n",
    "\n",
    "    normalized_hand_dictionary = {}\n",
    "    for idx in range(len(hand_landmarks_list)):\n",
    "\n",
    "      hand_landmarks = hand_landmarks_list[idx]\n",
    "      allpoints = np.array([ np.array((landmark.y, landmark.x)) for landmark in hand_landmarks])\n",
    "\n",
    "      #print(allpoints)\n",
    "      normalized_hand_dictionary[handedness_list[idx][0].category_name]=allpoints\n",
    "\n",
    "\n",
    "    # Transform nomalized hand landmarks into image coordinates\n",
    "    hand_dictionary = {cat:normalized_hand_dictionary[cat] * (src_color.shape[0], src_color.shape[1])\n",
    "                       for cat in normalized_hand_dictionary}\n",
    "\n",
    "    # flip back coordinates: try to fix this more sustainably later\n",
    "    hand_dictionary= {cat:np.flip(hand_dictionary[cat], axis=1) for cat in hand_dictionary}\n",
    "    \n",
    "    \n",
    "    if display==True:\n",
    "            \n",
    "        plt.figure(figsize=(15, 15))\n",
    "        plt.imshow(image, origin='lower')\n",
    "        for idx in range(hand_dictionary['Left'].shape[0]):\n",
    "            pointLeft = hand_dictionary['Left'][idx]\n",
    "            pointRight = hand_dictionary['Right'][idx]\n",
    "            plt.plot([pointLeft[0], pointRight[0]], [pointLeft[1], pointRight[1]], 'o',label=str(idx))\n",
    "        plt.legend()\n",
    "        plt.title(\"Hand landmark Legend\")\n",
    "        plt.show()\n",
    "    return hand_dictionary, detection_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#download off-the-shelf model\n",
    "\n",
    "!wget -q https://storage.googleapis.com/mediapipe-models/hand_landmarker/hand_landmarker/float16/1/hand_landmarker.task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": "true"
   },
   "source": [
    "## run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rh_and_lh_landmarks(VID_FILE, disp_frames=500, show = True):\n",
    "    # Use OpenCV’s VideoCapture to load the input video.\n",
    "    cap = cv2.VideoCapture(VID_FILE)\n",
    "    # Load the frame rate of the video using OpenCV’s CV_CAP_PROP_FPS\n",
    "    # You’ll need it to calculate the timestamp for each frame.\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "    fpms = fps/1000\n",
    "    print(\"frames per ms\", fpms)\n",
    "    total_num_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    print(\"total frames in video\", total_num_frames)\n",
    "\n",
    "\n",
    "    ## Downsample\n",
    "    ret = cap.set(cv2.CAP_PROP_FRAME_WIDTH,320)\n",
    "    ret = cap.set(cv2.CAP_PROP_FRAME_HEIGHT,240)\n",
    "    base_options = python.BaseOptions(model_asset_path='hand_landmarker.task')\n",
    "    options = vision.HandLandmarkerOptions(base_options=base_options,\n",
    "                                           running_mode=VisionRunningMode.VIDEO,\n",
    "                                           num_hands=2)\n",
    "    detector = vision.HandLandmarker.create_from_options(options)\n",
    "\n",
    "\n",
    "    rh_res = []\n",
    "    lh_res = []\n",
    "\n",
    "    # Loop through each frame in the video using VideoCapture#read()\n",
    "    cur_frame = 0\n",
    "    \n",
    "    time_start = time.time()\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if ret == False:\n",
    "            break\n",
    "        frame.flags.writeable = False\n",
    "        #need to convert to rgb before it can be processed\n",
    "        frame= cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        #downsample\n",
    "        frame = cv2.pyrDown(frame)\n",
    "       # print(frame.shape)\n",
    "\n",
    "\n",
    "        frame_timestamp_ms = int(cur_frame/fpms)\n",
    "\n",
    "\n",
    "        hand_dictionary, detection_result = get_hand_landmarks(frame, detector, \n",
    "                                   timestamp = frame_timestamp_ms,\n",
    "                                  display=False)\n",
    "\n",
    "        if cur_frame % disp_frames ==0:\n",
    "            if show:\n",
    "                frame.flags.writeable = True\n",
    "                annotated_image = draw_landmarks_on_image(frame, detection_result)\n",
    "                plt.figure(figsize=(20,10))\n",
    "                plt.imshow(annotated_image)\n",
    "                plt.show()\n",
    "            print(\"Time elasped, \",  time.time() - time_start, \"after  \", cur_frame*.033, \"seconds in video elpased\")\n",
    "        if 'Right' in hand_dictionary:\n",
    "            #Save hand landmark data        \n",
    "            rh_res.append(hand_dictionary['Right'])\n",
    "\n",
    "        else:\n",
    "            rh_res.append([])\n",
    "\n",
    "        if 'Left' in hand_dictionary:\n",
    "            lh_res.append(hand_dictionary['Left'])      \n",
    "        else:\n",
    "            lh_res.append([])\n",
    "        cur_frame = cur_frame + 1 #update\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    \n",
    "    return lh_res, rh_res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## analyze\n",
    "\n",
    "- hand labeling is funky, try full body model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": "true"
   },
   "source": [
    "### functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_clean_points(hand_take):\n",
    "    start = 0\n",
    "    clean_data = []\n",
    "    \n",
    "    for i in range(len(hand_take)):\n",
    "        el = hand_take[i]\n",
    " \n",
    "        \n",
    "        # video starts with no hands detected  \n",
    "        if  len(el)==0 and len(clean_data)==0:\n",
    "\n",
    "            start +=1   \n",
    "        #middle of video with no hands detected \n",
    "        elif len(el)==0:\n",
    "            clean_data.append(clean_data[-1])\n",
    "\n",
    "        #normal case\n",
    "        else:\n",
    "            clean_data.append(el)\n",
    "\n",
    "    return clean_data, start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#takes a list of detection results(single hand), returns valid indices and points\n",
    "#boundary frame is where one hand starts and other stops\n",
    "def divide_data_clean(res, boundary_frame):\n",
    "    \n",
    "    take1 = res[0:boundary_frame]\n",
    "    take2 = res[boundary_frame:len(res)]\n",
    "    \n",
    "    \n",
    "    t1_clean_data, t1_start= get_clean_points(take1)\n",
    "    t2_clean_data, t2_start = get_clean_points(take2)\n",
    "    \n",
    "    return {'t1':(t1_start, np.array(t1_clean_data)), 't2':(t2_start, np.array(t2_clean_data))}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": "true"
   },
   "source": [
    "### Front view (slightly right), RH only\n",
    "\n",
    "need to do rh and lh separately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VID_FILE = \"./videos/schuman_1_frontview_2takes.MOV\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boundary_frame = (60 + 23) * 30\n",
    "boundary_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rh_res, lh_res = get_rh_and_lh_landmarks(VID_FILE,disp_frames=50,  show=True)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "takes = divide_data_clean(lh_res, boundary_frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rh_t1, rh_t2 = takes['t1'][1], takes['t2'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize with wrist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrist_motion_t1  = rh_t1[:, 0, :]\n",
    "wrist_motion_t2  = rh_t2[:, 0, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(wrist_motion_t1[:, 0] )\n",
    "plt.plot(wrist_motion_t2[:, 0]-100 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(wrist_motion_t1[:, 1] )\n",
    "plt.plot(wrist_motion_t2[:, 1] -200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5, rh_pnts.shape[1]):\n",
    "    \n",
    "\n",
    "    \n",
    "    t1_landmark = rh_t1[:, i, :] - wrist_motion_t1\n",
    "    t2_landmark = rh_t2[:, i, :] - wrist_motion_t2\n",
    "    \n",
    "    plt.figure(figsize=(15,10))\n",
    "    plt.title(\"Landmark Y \"+ str(i))\n",
    "    plt.plot(t1_landmark[:,1] )\n",
    "    plt.plot(t2_landmark[:,1]- 100 , alpha = .5)\n",
    "   \n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    plt.figure(figsize=(20,10))\n",
    "    plt.title(\"Landmark X \"+ str(i))\n",
    "    plt.plot(t1_landmark[:,0] )\n",
    "    plt.plot(t2_landmark[:,0]- 100 , alpha = .5)\n",
    "   \n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": "true"
   },
   "source": [
    "### Side view, RH only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VID_FILE = \"./videos/schuman_1_sideview.MOV\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boundary_frame = (60 + 34) * 30\n",
    "boundary_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rh_res, lh_res = get_rh_and_lh_landmarks(VID_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "takes = divide_data_clean(lh_res, boundary_frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rh_t1, rh_t2 = takes['t1'][1], takes['t2'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrist_motion_t1  = rh_t1[:, 0, :]\n",
    "wrist_motion_t2  = rh_t2[:, 0, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(wrist_motion_t1[:, 0] )\n",
    "plt.plot(wrist_motion_t2[:, 0]-100 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(wrist_motion_t1[:, 1] )\n",
    "plt.plot(wrist_motion_t2[:, 1] -200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5, rh_pnts.shape[1]):\n",
    "    \n",
    "\n",
    "    \n",
    "    t1_landmark = rh_t1[:, i, :] - wrist_motion_t1\n",
    "    t2_landmark = rh_t2[:, i, :] - wrist_motion_t2\n",
    "    \n",
    "    plt.figure(figsize=(15,10))\n",
    "    plt.title(\"Landmark Y \"+ str(i))\n",
    "    plt.plot(t1_landmark[:,1] )\n",
    "    plt.plot(t2_landmark[:,1]- 100 , alpha = .5)\n",
    "   \n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    plt.figure(figsize=(20,10))\n",
    "    plt.title(\"Landmark X \"+ str(i))\n",
    "    plt.plot(t1_landmark[:,0] )\n",
    "    plt.plot(t2_landmark[:,0]- 100 , alpha = .5)\n",
    "   \n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Full body landmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mediapipe\n",
    "mp_drawing = mediapipe.solutions.drawing_utils\n",
    "mp_drawing_styles = mediapipe.solutions.drawing_styles\n",
    "mp_holistic =mediapipe.solutions.holistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_generic_landmarks(landmarks):\n",
    "\n",
    "  allpoints = np.array([ np.array((landmark.y, landmark.x)) for landmark in landmarks])\n",
    "\n",
    "  return allpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this should be integrated into online pipeline but ok as separate function for now\n",
    "def process_raw_landmarks(all_landmarks):\n",
    "    all_pose_dct = {'RH':[],\n",
    "                     'LH':[],\n",
    "                     'Pose': [],\n",
    "                     'Face': []}\n",
    "    for timestep in all_landmarks:\n",
    "        #print(\"*********\")\n",
    "        res_types = {'RH':timestep.right_hand_landmarks,\n",
    "                     'LH':timestep.left_hand_landmarks,\n",
    "                     'Pose': timestep.pose_landmarks,\n",
    "                     'Face': timestep.face_landmarks\n",
    "                    }\n",
    "        for key in res_types:\n",
    "            if res_types[key]== None:\n",
    "                res_types[key]= []\n",
    "            else:\n",
    "                #print(len(res_types[key].landmark))\n",
    "                res_types[key]= get_generic_landmarks(res_types[key].landmark)\n",
    "\n",
    "\n",
    "        for key in all_pose_dct:\n",
    "            all_pose_dct[key].append(res_types[key])\n",
    "\n",
    "\n",
    "    all_pose_arrs = {}\n",
    "\n",
    "    for key in all_pose_dct:\n",
    "        data, start = get_clean_points(all_pose_dct[key])\n",
    "\n",
    "        all_pose_arrs[key]= {'data': np.array(data), 'start':start}\n",
    "\n",
    "    return all_pose_arrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def process_video_all_landmarks(VID_FILE, display_freq = 500, show= True, lim_len = None):\n",
    "    # Use OpenCV’s VideoCapture to load the input video.\n",
    "    cap = cv2.VideoCapture(VID_FILE )\n",
    "    # Load the frame rate of the video using OpenCV’s CV_CAP_PROP_FPS\n",
    "    # You’ll need it to calculate the timestamp for each frame.\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    fpms = fps/1000\n",
    "    print(\"frames per ms\", fpms)\n",
    "    total_num_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    print(\"total frames in video\", total_num_frames)\n",
    "    ## get width and height\n",
    "    frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    all_landmarks = []\n",
    "\n",
    "\n",
    "    idx = 0\n",
    "    time_start = time.time()\n",
    "    with mp_holistic.Holistic(\n",
    "        min_detection_confidence=0.5,\n",
    "        min_tracking_confidence=0.5) as holistic:\n",
    "      while idx < total_num_frames:\n",
    "\n",
    "        if idx %100 ==0:\n",
    "          print(\"progess:\", idx/total_num_frames, 'on frame', idx)\n",
    "        success, image = cap.read()\n",
    "        if not success:\n",
    "          print(\"Ignoring empty camera frame.\")\n",
    "          # If loading a video, use 'break' instead of 'continue'.\n",
    "          continue\n",
    "\n",
    "\n",
    "        #visualize\n",
    "\n",
    "        # To improve performance, optionally mark the image as not writeable to\n",
    "        # pass by reference.\n",
    "        image.flags.writeable = False\n",
    "\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        #model_start = time.time()\n",
    "        results = holistic.process(image)\n",
    "        #print('model took', time.time()-model_start)\n",
    "        all_landmarks.append(results)\n",
    "\n",
    "\n",
    "        if idx% display_freq ==0:\n",
    "\n",
    "            print(\"Time elasped, \",  time.time() - time_start, \"after  \", idx/60, \"seconds in video elpased\")\n",
    "            if show:\n",
    "                image.flags.writeable = True\n",
    "                image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "                mp_drawing.draw_landmarks(\n",
    "                    image,\n",
    "                    results.face_landmarks,\n",
    "                    mp_holistic.FACEMESH_CONTOURS,\n",
    "                    landmark_drawing_spec=None,\n",
    "                    connection_drawing_spec=mp_drawing_styles\n",
    "                    .get_default_face_mesh_contours_style())\n",
    "                mp_drawing.draw_landmarks(\n",
    "                    image,\n",
    "                    results.pose_landmarks,\n",
    "                    mp_holistic.POSE_CONNECTIONS,\n",
    "                    landmark_drawing_spec=mp_drawing_styles\n",
    "                    .get_default_pose_landmarks_style())\n",
    "                mp_drawing.draw_landmarks(\n",
    "                    image,\n",
    "                    results.left_hand_landmarks,\n",
    "                    mp_holistic.HAND_CONNECTIONS,\n",
    "                    landmark_drawing_spec=mp_drawing_styles\n",
    "                    .get_default_hand_landmarks_style())\n",
    "\n",
    "                mp_drawing.draw_landmarks(\n",
    "                    image,\n",
    "                    results.right_hand_landmarks,\n",
    "                    mp_holistic.HAND_CONNECTIONS,\n",
    "                    landmark_drawing_spec=mp_drawing_styles\n",
    "                    .get_default_hand_landmarks_style())\n",
    "                #save\n",
    "                plt.figure(figsize=(15, 15))\n",
    "                plt.imshow(image)\n",
    "                plt.show()\n",
    "\n",
    "\n",
    "\n",
    "        if idx==lim_len:\n",
    "            break\n",
    "        idx = idx+1\n",
    "\n",
    "\n",
    "\n",
    "    cap.release()\n",
    "\n",
    "    return process_raw_landmarks(all_landmarks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vis_landmarks_with_midi_1d(all_landmarks_list, #list of landmarks\n",
    "                           note_times_df_lst=None, #when (in frames) each note occurs \n",
    "                           norm_landmark_list=[], #assume no normalization\n",
    "                            vis_landmark_idx=0,\n",
    "                            vis_landmark_dim = 1,\n",
    "                            vis_landmark_name= 'RH',\n",
    "                            colors = ['red', 'blue'],\n",
    "                            label = 'RH wrist yloc',#interp this eventually, for now assume 2 takes\n",
    "                            shift_del = 0, #we can alter y axis location of differt takes for smoother visualization\n",
    "                            axis_change = 0,\n",
    "                            text_shift = .01, \n",
    "                            label_text = True,\n",
    "                            label_axis = True,\n",
    "                            seq_pch = '.'\n",
    "                           ):\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "        \n",
    "    #plot landmark sequence\n",
    "    for i in range(len(all_landmarks_list)):\n",
    "        \n",
    "    \n",
    "        #visualization scheme: every landmark of one take is drawn first in the same color        \n",
    "        plot_color = colors[i]\n",
    "        all_landmarks_in_take = all_landmarks_list[i]\n",
    "        \n",
    "        #extract the sequence to plot\n",
    "        seq = all_landmarks_in_take[vis_landmark_name]['data'][:, vis_landmark_idx, vis_landmark_dim]\n",
    "            \n",
    "        if len(norm_landmark_list) != 0:\n",
    "            \n",
    "            if len(seq)== len(norm_landmark_list[i]):\n",
    "                seq = seq-norm_landmark_list[i]\n",
    "            else:\n",
    "                \n",
    "                print(\"shape of seq\", seq.shape)\n",
    "                print(\"shape of norm landmarks\", norm_landmark_list[i].shape)\n",
    "                print(\"Check dims, there must be a one to one relation between normalization shift and data value\")\n",
    "                return\n",
    "        \n",
    "            \n",
    "       \n",
    "        \n",
    "        #plot landmarks \n",
    "        seq = seq - axis_change\n",
    "        xstart = all_landmarks_in_take[vis_landmark_name]['start']\n",
    "        frame_seq = np.arange(0, len(seq))+xstart\n",
    "        plt.plot(frame_seq, seq, seq_pch , label =  \"Take \"+str(i)+\": \"+label, color = plot_color)\n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "        #plot note times:\n",
    "        if note_times_df_lst != None:\n",
    "            \n",
    "            note_times_df = note_times_df_lst[i]\n",
    "            \n",
    "            \n",
    "\n",
    "            #determine which notes to plot\n",
    "            \n",
    "            notestart_time = int(note_times_df.iloc[0]['vidframe'])\n",
    "            print(notestart_time)\n",
    "            \n",
    " \n",
    "            noteend_time = xstart + len(seq)\n",
    "            print(noteend_time)\n",
    "            \n",
    "            all_note_times = note_times_df['vidframe']\n",
    "            selected_note_times =all_note_times[all_note_times < noteend_time]\n",
    "            \n",
    "            \n",
    "            marker_min_height = min(seq[notestart_time: noteend_time])- 2*text_shift\n",
    "            marker_max_height  = max(seq[notestart_time: noteend_time])+2*text_shift\n",
    "        \n",
    "\n",
    "            if label_axis:\n",
    "                print(\"axis labeled, i is \", i)\n",
    "                plt.vlines(selected_note_times, marker_min_height, \n",
    "                       marker_max_height, color = plot_color, label = \"Take \"+str(i)+\": note onset times\")\n",
    "            else:\n",
    "                plt.vlines(selected_note_times, marker_min_height, \n",
    "                       marker_max_height, color = plot_color)\n",
    "\n",
    "\n",
    "            #label with note index\n",
    "            if label_text:\n",
    "                for j in range(0, len(selected_note_times)):#len(note_times_df['vidframe'])):\n",
    "\n",
    "                    #print(note_times_df['vidframe'][j])\n",
    "                    plt.text( selected_note_times[j], marker_min_height-textshift,  str(j), color = plot_color)\n",
    "\n",
    "    \n",
    "    \n",
    "        axis_change = axis_change - shift_del\n",
    "\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "view = 'front'\n",
    "take = '1'\n",
    "VID_FILE1 = os.path.join(\"./videos\", view, view+\"_take\"+take, view+\"_take\"+take+'.mov')\n",
    "perf_atm1 = os.path.join(\"./videos\", view, view+\"_take\"+take, view+\"_take\"+take+'.atm')\n",
    "\n",
    "note_position_df1 = get_alignment_from_orchestra(perf_atm1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_landmarks_1 = process_video_all_landmarks(VID_FILE1, display_freq = 1000, show= True, lim_len = 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "take = '2'\n",
    "VID_FILE2 = os.path.join(\"./videos\", view, view+\"_take\"+take, view+\"_take\"+take+'.mov')\n",
    "print(\"file path:\", VID_FILE2)\n",
    "perf_atm2 = os.path.join(\"./videos\", view, view+\"_take\"+take, view+\"_take\"+take+'.atm')\n",
    "\n",
    "note_position_df2 = get_alignment_from_orchestra(perf_atm2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_landmarks_2 = process_video_all_landmarks(VID_FILE2, display_freq = 1000, show= True, lim_len = 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "                           norm_landmark_list=[], #assume no normalization\n",
    "                            vis_landmark_idx=0,\n",
    "                            vis_landmark_dim = 1,\n",
    "                            vis_landmark_name= 'RH',"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "vis_landmarks_with_midi_1d([processed_landmarks_1, processed_landmarks_2], [note_position_df1, note_position_df2],\n",
    "                           vis_landmark_name = 'LH', shift_del = .12,  label= \"LH wrist yloc\")\n",
    "\n",
    "#vis_landmarks_with_midi_1d([processed_landmarks_1, processed_landmarks_2], [note_position_df1, note_position_df2],\n",
    "#                           vis_landmark_name = 'RH', shift_del = .12,  label_text = True,\n",
    "#                            label_axis = False, seq_pch = '+', axis_change = .1, label= \"RH wrist yloc\")\n",
    "\n",
    "plt.ylim(.38, .6)\n",
    "plt.xlim(200, None)\n",
    "\n",
    "plt.title(\"Left Hand Wrist Motion\")\n",
    "plt.savefig(\"left_hand_wrist_schuman1_front.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_take_landmarks = [processed_landmarks_1, processed_landmarks_2]\n",
    "LH_wrist_x = [take['LH']['data'][:, 0, 0] for take in multi_take_landmarks]\n",
    "LH_wrist_y = [take['LH']['data'][:, 0, 1] for take in multi_take_landmarks]\n",
    "RH_wrist_x = [take['RH']['data'][:, 0, 0] for take in multi_take_landmarks]\n",
    "RH_wrist_y = [take['RH']['data'][:, 0, 1] for take in multi_take_landmarks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.figure(figsize=(20,10))\n",
    "vis_landmarks_with_midi_1d(multi_take_landmarks, \n",
    "                           [note_position_df1, note_position_df2],\n",
    "                           LH_wrist_y,\n",
    "                            vis_landmark_idx=20,\n",
    "                            vis_landmark_dim = 1,\n",
    "                           vis_landmark_name = 'LH', \n",
    "                           shift_del = .12, \n",
    "                           label = 'LH pinky yloc')\n",
    "\n",
    "#vis_landmarks_with_midi_1d([processed_landmarks_1, processed_landmarks_2], [note_position_df1, note_position_df2],\n",
    "#                           vis_landmark_name = 'RH', shift_del = .12,  label_text = True,\n",
    "#                            label_axis = False, seq_pch = '+', axis_change = .1)\n",
    "\n",
    "plt.ylim(-.12, .07)\n",
    "\n",
    "plt.title(\"Left Hand Pinky fingertip - Y location\\n(Normalized by left wrist y position)\")\n",
    "plt.xlim(200, None)\n",
    "\n",
    "plt.savefig(\"left_hand_pinky_tip_yloc_schuman1_front.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.figure(figsize=(20,10))\n",
    "vis_landmarks_with_midi_1d(multi_take_landmarks, \n",
    "                           [note_position_df1, note_position_df2],\n",
    "                           LH_wrist_x,\n",
    "                            vis_landmark_idx=8,\n",
    "                            vis_landmark_dim = 0,\n",
    "                           vis_landmark_name = 'LH', shift_del = .12, label = 'LH middle fingertip xloc')\n",
    "\n",
    "#vis_landmarks_with_midi_1d([processed_landmarks_1, processed_landmarks_2], [note_position_df1, note_position_df2],\n",
    "#                           vis_landmark_name = 'RH', shift_del = .12,  label_text = True,\n",
    "#                            label_axis = False, seq_pch = '+', axis_change = .1)\n",
    "\n",
    "plt.ylim(-.155, .07)\n",
    "\n",
    "plt.title(\"Left Hand middle fingertip - X location\\nNormalized by wrist\")\n",
    "plt.xlim(200, None)\n",
    "\n",
    "plt.savefig(\"left_hand_middle_fingertip_schuman1_front.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.figure(figsize=(20,10))\n",
    "vis_landmarks_with_midi_1d(multi_take_landmarks, \n",
    "                           [note_position_df1, note_position_df2],\n",
    "                            vis_landmark_idx=0,\n",
    "                            vis_landmark_dim = 1,\n",
    "                           vis_landmark_name = 'RH', shift_del = .12, label = 'Nose xloc')\n",
    "\n",
    "#vis_landmarks_with_midi_1d([processed_landmarks_1, processed_landmarks_2], [note_position_df1, note_position_df2],\n",
    "#                           vis_landmark_name = 'RH', shift_del = .12,  label_text = True,\n",
    "#                            label_axis = False, seq_pch = '+', axis_change = .1)\n",
    "\n",
    "plt.ylim(.3, .6)\n",
    "\n",
    "#plt.title(\"Left Hand middle fingertip - X location\\nNormalized by wrist\")\n",
    "#plt.xlim(200, None)\n",
    "\n",
    "#plt.savefig(\"left_hand_middle_fingertip_schuman1_front.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "vis_landmarks_with_midi_1d([processed_landmarks_1, processed_landmarks_2], \n",
    "                           vis_landmark_name = 'LH', shift_del = .1)\n",
    "\n",
    "\n",
    "note_times_df = note_position_df1\n",
    "\n",
    "marker_min = .4\n",
    "marker_max = .5\n",
    "plot_color = 'red'\n",
    "textshift = .01\n",
    "\n",
    "plot_notes = range(0,20)\n",
    "\n",
    "plt.vlines(note_times_df['vidframe'], marker_min, marker_max, color = plot_color)\n",
    "for j in plot_notes:#len(note_times_df['vidframe'])):\n",
    "    #print(note_times_df['vidframe'][j])\n",
    "    plt.text( note_times_df['vidframe'][j], marker_min-textshift,  str(j), color = plot_color)\n",
    "\n",
    "\n",
    "note_times_df = note_position_df2\n",
    "\n",
    "marker_min = .5\n",
    "marker_max = .6\n",
    "plot_color = 'blue'\n",
    "\n",
    "plt.vlines(note_times_df['vidframe'], marker_min, marker_max, color = plot_color)\n",
    "for j in plot_notes:#len(note_times_df['vidframe'])):\n",
    "    #print(note_times_df['vidframe'][j])\n",
    "    plt.text( note_times_df['vidframe'][j], marker_min-textshift,  str(j), color = plot_color)\n",
    "\n",
    "plt.xlim(200, 1000)\n",
    "plt.ylim(.35, .7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "note_position_df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_landmarks_2['RH']['data'][:, 4, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Functions\n",
    "\n",
    "Run this first"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Get frame alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not local:\n",
    "    vid_pth = \"/home/intern-2023-kaitlin/data/dataset/burgmuller-performances-from-top/b-01.mp4\"\n",
    "else:\n",
    "    vid_pth = \"../data/dataset/testing/b-01.mp4\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_boundary_frame(vid_pth, time_of_boundary, time_of_ref): #time of frame in seconds\n",
    "    cap = cv2.VideoCapture(vid_pth)\n",
    "    total_num_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    \n",
    "    print(total_num_frames)\n",
    "    boundary_frame = int(time_of_boundary*fps)\n",
    "    ref_hand_frame = int(time_of_ref*fps)\n",
    "    print(boundary_frame )\n",
    "    i = 0\n",
    "    while i < boundary_frame :\n",
    "        ret, src_color = cap.read()\n",
    "        \n",
    "        if i== ref_hand_frame:\n",
    "            hand_ref_frame = src_color.copy()\n",
    "            plt.title(\"frame used for identifying hand positions at frame:\"+str(i))\n",
    "            plt.imshow(prepare_frame(hand_ref_frame), origin='lower')\n",
    "            plt.show()\n",
    "        i=i+1\n",
    "    plt.title(\"Boundary frame \"+str(boundary_frame))\n",
    "    plt.imshow(prepare_frame(src_color), origin='lower')\n",
    "    plt.show()\n",
    "    alignment_computation_frame = src_color.copy()   \n",
    "    cap.release()\n",
    "    \n",
    "    \n",
    "    return hand_ref_frame, alignment_computation_frame, boundary_frame, fps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Functions to obtain warp\n",
    "\n",
    "- Inputs: \n",
    "    - path to template (should be hardcoded)\n",
    "    - path to frame (in video would be inputted differently)\n",
    "    - detector (mediapipe detector object, created above)\n",
    "    - middle finger notes - Notes being played by the 2 middle fingers. This could be extracted automatically, but in practice probably easier as a user directive (i.e. play these notes with these fingers to initialize system)\n",
    "    - display - boolean for visual output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From align_movie.ipynb notebook\n",
    "def image_preprocessing_pipeline(img):    \n",
    "    '''画像の前処理． グレースケールに変換する. できるだけ２つの画像の相関を高めるような操作にする．\n",
    "    今回の場合，鍵盤の白黒がはっきりしているから，白黒を見るだけで相関が高くなると予想．'''\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    normalizedImg = np.zeros(img.shape)\n",
    "    normalizedImg = cv2.normalize(img,  normalizedImg, 0, 255, cv2.NORM_MINMAX)\n",
    "    img = normalizedImg\n",
    "    return img\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_frame(src_color):\n",
    "    \n",
    "    return cv2.flip(src_color, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "'''\n",
    "template_color: template of piano (file provided)\n",
    "src_color: image of hands on piano- used to find hand landmarks (should be start frame of original video)\n",
    "middle_finger_notes: the notes played by middle fingers on each hand in src_color\n",
    "detector: mediapipe hand landmarker detector\n",
    "alignment_computation_frame: image of pinao used for template matching ( a different frame from origianl video)\n",
    "'''\n",
    "def get_piano_alignment_transform(template_color, src_color, \n",
    "                                  alignment_computation_frame ,\n",
    "                                  detector, middle_finger_notes, \n",
    "                                  display=False, prevComputedInitialization = []):\n",
    "    '''\n",
    "    Preprocessing (rotate image to unified perspective of person's view of piano while playing)  \n",
    "    '''\n",
    "    src_color_display = prepare_frame(cv2.cvtColor(src_color, cv2.COLOR_BGR2RGB))\n",
    "    src_color = prepare_frame(src_color)\n",
    "    alignment_computation_frame = prepare_frame(alignment_computation_frame)\n",
    "    \n",
    "    '''\n",
    "    Identify hand positions\n",
    "    '''\n",
    "    hand_dictionary = get_hand_landmarks(src_color, detector, \n",
    "                                         detector_type = 'image', \n",
    "                                         display=display)\n",
    "    \n",
    "    #locations of middle fingertips\n",
    "    rh_middle_fingertip = hand_dictionary[\"Right\"][MIDDLE_FINGERTIP_IDX]\n",
    "    lh_middle_fingertip = hand_dictionary[\"Left\"][MIDDLE_FINGERTIP_IDX]\n",
    "    print(\"fingertip absolute locations\", rh_middle_fingertip, lh_middle_fingertip)\n",
    "    #notes being played (unit, position on template). WE ASSUME THIS IS CLOSE TO WHITE-BLACK KEY BOUNDARY\n",
    "    rh_middle_finger_notepos = middle_finger_notes[1]*T_WIDTH/WHITE_KEYS_ON_PIANO\n",
    "    lh_middle_finger_notepos = middle_finger_notes[0]*T_WIDTH/WHITE_KEYS_ON_PIANO\n",
    "    print(\"fingertip positions on template\", rh_middle_finger_notepos, lh_middle_finger_notepos)\n",
    "    \n",
    "    '''\n",
    "    Extract score-based keypoints in image\n",
    "    '''\n",
    "    middle_finger_interval = middle_finger_notes[1] - middle_finger_notes[0]\n",
    "    print(\"middle finger interval is \", middle_finger_interval)\n",
    "    middle_finger_dist = np.sqrt(np.sum(lh_middle_fingertip-rh_middle_fingertip)**2)\n",
    "    print(\"middle finger distance in image is \", middle_finger_dist)    \n",
    "    \n",
    "    '''\n",
    "    Assign corresponding keypoints in template\n",
    "    '''\n",
    "    \n",
    "    template_lh_middle_fingertip = np.array([lh_middle_finger_notepos, BLACK_KEY_BOTTOM_RATIO*T_HEIGHT])\n",
    "    template_rh_middle_fingertip = np.array([rh_middle_finger_notepos, BLACK_KEY_BOTTOM_RATIO*T_HEIGHT])\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    Assign Middle C position on template (bottom and top of note\n",
    "    '''\n",
    "    midpoint_note_pose = int(middle_finger_notes[0]+middle_finger_notes[1])/2+0.5\n",
    "    print(\"new midpoint:\", midpoint_note_pose)\n",
    "    template_middle_c_top = np.array([midpoint_note_pose/WHITE_KEYS_ON_PIANO*T_WIDTH, T_HEIGHT])\n",
    "    template_middle_c_bottom = np.array([midpoint_note_pose/WHITE_KEYS_ON_PIANO*T_WIDTH, 0] )\n",
    "\n",
    "    \n",
    "    '''\n",
    "    Estimate length and width of piano\n",
    "    '''\n",
    "    src_piano_length_estimate = middle_finger_dist * WHITE_KEYS_ON_PIANO /middle_finger_interval\n",
    "    print(\"piano length estimate\", src_piano_length_estimate )\n",
    "    src_piano_width_estimate = src_piano_length_estimate * T_HEIGHT/T_WIDTH\n",
    "    print(\"piano width estimate\", src_piano_width_estimate)\n",
    "    black_key_height_estimate = (1-BLACK_KEY_BOTTOM_RATIO)*src_piano_width_estimate\n",
    "    under_black_key_height_estimate = BLACK_KEY_BOTTOM_RATIO*src_piano_width_estimate\n",
    "    \n",
    "    '''\n",
    "    Extract expected position of Middle C on image\n",
    "    '''\n",
    "    unit_middle_finger_vector = (rh_middle_fingertip-lh_middle_fingertip)/middle_finger_dist\n",
    "    print(\"middle finger unit vecotr\", unit_middle_finger_vector)\n",
    "    c_from_left_interval =  midpoint_note_pose - middle_finger_notes[0]\n",
    "    print(\"c_from_left_interval\", c_from_left_interval)\n",
    "    c_from_left_dst = middle_finger_dist*c_from_left_interval/middle_finger_interval\n",
    "    print(\"distance of middle c from left finger is\", c_from_left_dst)\n",
    "    #estimate location of middle c by extending vector connecting two middle fingertips\n",
    "    middle_c_blackey_edge = lh_middle_fingertip+ c_from_left_dst*unit_middle_finger_vector\n",
    "    #use tangent vector to estimate locations of top and bottom of key\n",
    "    unit_middle_finger_tangent_vector = np.array([unit_middle_finger_vector[1], \n",
    "                                              -1* unit_middle_finger_vector[0]])\n",
    "    middle_c_top = middle_c_blackey_edge - black_key_height_estimate * unit_middle_finger_tangent_vector\n",
    "    middle_c_bottom = middle_c_blackey_edge +under_black_key_height_estimate*unit_middle_finger_tangent_vector\n",
    "    \n",
    "    '''\n",
    "    Warp from image to template\n",
    "    '''\n",
    "    \n",
    "    #image\n",
    "    x_tgt = np.array([rh_middle_fingertip[0], middle_c_top[0], lh_middle_fingertip[0], middle_c_bottom[0]])\n",
    "    y_tgt = np.array([rh_middle_fingertip[1], middle_c_top[1], lh_middle_fingertip[1], middle_c_bottom[1]])\n",
    "    Xt = np.c_[x_tgt,y_tgt].astype('float32')\n",
    "    \n",
    "    #template\n",
    "    x_ref = np.array([template_rh_middle_fingertip[0], template_middle_c_top[0], template_lh_middle_fingertip[0], template_middle_c_bottom[0]])\n",
    "    y_ref = np.array([template_rh_middle_fingertip[1], template_middle_c_top[1], template_lh_middle_fingertip[1], template_middle_c_bottom[1]])\n",
    "    Xr = np.c_[x_ref,y_ref].astype('float32')    \n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    # Use the above info to find the transform matrix\n",
    "\n",
    "    if len(prevComputedInitialization)>0:\n",
    "        warp_initial = prevComputedInitialization\n",
    "    else:\n",
    "        warp_initial = cv2.getPerspectiveTransform(Xt, Xr).astype('float32')\n",
    "\n",
    "\n",
    "    \n",
    "    '''\n",
    "    Visualize (sanity check)\n",
    "    '''\n",
    "    if display:\n",
    "        #image keypoints\n",
    "        plt.imshow(src_color_display, origin='lower')\n",
    "        plt.plot(x_tgt, y_tgt, 'r')\n",
    "        plt.title(\"keypoints on frame\")\n",
    "        plt.show()\n",
    "        \n",
    "        #template keypoints\n",
    "        plt.imshow(template_color, origin='lower')\n",
    "        plt.plot(x_ref, y_ref, 'r')\n",
    "        plt.title(\"keyframes on template\")\n",
    "        plt.show()\n",
    "        \n",
    "        #initial warp alignment, SHOULD BE PRETTY CLOSE\n",
    "        initial_warped_frame =  cv2.warpPerspective(src_color, \n",
    "                                            warp_initial, (src_color.shape[1], src_color.shape[0]))\n",
    "        plt.imshow(template_color, alpha=.5, origin='lower')\n",
    "\n",
    "        plt.imshow(initial_warped_frame, alpha = .5, origin='lower')\n",
    "        plt.xlim(0, T_WIDTH)\n",
    "        plt.ylim(0, T_HEIGHT)\n",
    "        plt.title(\"Initial alignment, SHOULD BE CLOSE\")\n",
    "        plt.show()\n",
    "        \n",
    "    '''\n",
    "    Refine with findECC\n",
    "    '''\n",
    "    \n",
    "    #create mask for template(I think this makes id easier)\n",
    "    t_mask = np.zeros((template_color.shape[0], template_color.shape[1]))\n",
    "    \n",
    "    #versions\n",
    "    t_mask[:, int(template_lh_middle_fingertip[0])-200:int(template_rh_middle_fingertip[0])+200]=1\n",
    "    #t_mask[:, int(template_lh_middle_fingertip[0]):int(template_rh_middle_fingertip[0])]=1    \n",
    "    t_mask[:, int(template_lh_middle_fingertip[0]-200):]=1     \n",
    "    \n",
    "    t_mask= t_mask.astype(np.uint8)\n",
    "    if display:\n",
    "        plt.imshow(t_mask*image_preprocessing_pipeline(template_color), origin='lower')\n",
    "        plt.show()\n",
    "    \n",
    "    (retval, warpMatrix) = cv2.findTransformECC(image_preprocessing_pipeline(alignment_computation_frame),\n",
    "                                            image_preprocessing_pipeline(template_color),\n",
    "                                           warp_initial  , cv2.MOTION_HOMOGRAPHY,\n",
    "                                            criteria, inputMask=t_mask, gaussFiltSize=5)## GOT RID OF MASK\n",
    "    \n",
    "    if display:\n",
    "        img2w = cv2.warpPerspective(src_color, warpMatrix, (T_WIDTH, T_HEIGHT))\n",
    "        img2w = cv2.warpPerspective(src_color, warpMatrix, (src_color.shape[1], src_color.shape[0]))\n",
    "        \n",
    "        plt.imshow(img2w, origin='lower')\n",
    "        plt.show()\n",
    "        plt.imshow(img2w, origin='lower')\n",
    "        plt.imshow(template_color, origin='lower', alpha=0.4)\n",
    "        plt.title(\"Final alignment of piano\")\n",
    "        plt.show()\n",
    "        print(\"in get matrix, final_wrp is \\n\", warpMatrix)        \n",
    "    \n",
    "    return warpMatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hand_landmarks(src_color, detector, detector_type = 'video', timestamp=None, display=False):\n",
    "\n",
    "\n",
    "    #make sure color space is correct before converting to mediapipe readable image type\n",
    "    image= cv2.cvtColor(src_color, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    if detector_type =='image':\n",
    "        mp_image = mp.Image(image_format=mp.ImageFormat.SRGB, data=image)\n",
    "\n",
    "        # STEP 4: Detect hand landmarks from the input image.\n",
    "        detection_result = detector.detect(mp_image)\n",
    "    elif detector_type == 'video':\n",
    "\n",
    "        mp_image = mp.Image(image_format=mp.ImageFormat.SRGB, data=image)\n",
    "        detection_result = detector.detect_for_video(mp_image, timestamp)\n",
    "    else:\n",
    "        print(\"invalid detector type, should be \\n\\\"image\\\"\\n or \\n\\\"video\\\"\\n\")\n",
    "    # Extract normalized hand landmarks\n",
    "    hand_landmarks_list = detection_result.hand_landmarks\n",
    "    handedness_list = detection_result.handedness\n",
    "\n",
    "    normalized_hand_dictionary = {}\n",
    "    for idx in range(len(hand_landmarks_list)):\n",
    "\n",
    "      hand_landmarks = hand_landmarks_list[idx]\n",
    "      allpoints = np.array([ np.array((landmark.y, landmark.x)) for landmark in hand_landmarks])\n",
    "\n",
    "      #print(allpoints)\n",
    "      normalized_hand_dictionary[handedness_list[idx][0].category_name]=allpoints\n",
    "\n",
    "\n",
    "    # Transform nomalized hand landmarks into image coordinates\n",
    "    hand_dictionary = {cat:normalized_hand_dictionary[cat] * (src_color.shape[0], src_color.shape[1])\n",
    "                       for cat in normalized_hand_dictionary}\n",
    "\n",
    "    # flip back coordinates: try to fix this more sustainably later\n",
    "    hand_dictionary= {cat:np.flip(hand_dictionary[cat], axis=1) for cat in hand_dictionary}\n",
    "    \n",
    "    \n",
    "    if display==True:\n",
    "            \n",
    "        plt.figure(figsize=(15, 15))\n",
    "        plt.imshow(image, origin='lower')\n",
    "        for idx in range(hand_dictionary['Left'].shape[0]):\n",
    "            pointLeft = hand_dictionary['Left'][idx]\n",
    "            pointRight = hand_dictionary['Right'][idx]\n",
    "            plt.plot([pointLeft[0], pointRight[0]], [pointLeft[1], pointRight[1]], 'o',label=str(idx))\n",
    "        plt.legend()\n",
    "        plt.title(\"Hand landmark Legend\")\n",
    "        plt.show()\n",
    "    return hand_dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Functions for applying warping\n",
    "\n",
    "- assume warpPerspective (4 points)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_warp(src,warp_mat, display=False):\n",
    "    #Warp image\n",
    "\n",
    "    #print(\"in compute perform warp, final_wrp is \\n\", warp_mat)\n",
    "    warp_dst = cv2.warpPerspective(src, warp_mat, ( src.shape[1], src.shape[0]))\n",
    "\n",
    "\n",
    "    if display:\n",
    "            \n",
    "        plt.imshow(warp_dst, origin='lower')\n",
    "        plt.title(\"Warped image\")\n",
    "        plt.show()\n",
    "    return warp_dst"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WarpAffine can be performed with matrix multiplication, but warp perspective is harder to express using matrix algebra. I should look into this though, it will help any type of optimization\n",
    "\n",
    "- Need to add extra dimension for bias in point terms: https://theailearner.com/tag/cv2-getaffinetransform/\n",
    "- From the documentation here(https://docs.opencv.org/4.x/da/d54/group__imgproc__transform.html#gaf73673a7e8e18ec6963e3774e6a94b87), the warpPerspective() matrix is applied thus:\n",
    "    $$dst(x,y) = src(\\frac{M_{11}x +M_{12}y+M_{13}}{M_{31}x+M_{32}y+M_{33}}, \n",
    "    \\frac{M_{21}x+M_{22}y+M_{23}}{M_{31}x+M_{32}y+M_{33}})$$\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#assume input format of hand array is \n",
    "\n",
    "'''\n",
    "\n",
    "[[key1x, key1y],\n",
    " [key2x, key2y]\n",
    " ...]\n",
    "\n",
    "'''\n",
    "def manual_warp_points(point_arr, M):\n",
    "\n",
    "    #print(\"in manual warp points, final_wrp is \\n\", M)\n",
    "    x = point_arr[:,0:1]\n",
    "    y = point_arr[:,1:2]\n",
    "    #print(\"stophere, \", point_arr)\n",
    "    norm_factor = M[2,0]*x + M[2,1]*y+M[2,2]\n",
    "\n",
    "    ret_x = M[0,0]*x + M[0,1]*y + M[0,2]\n",
    "    ret_y = M[1,0]*x + M[1,1]*y+ M[1,2]\n",
    "\n",
    "    non_norm_warped_points = np.concatenate((ret_x, ret_y), axis=1)\n",
    "\n",
    "    warped_points = (non_norm_warped_points/norm_factor).T\n",
    "    \n",
    "    return warped_points\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_transformed_hand_points(hand_dictionary, warp_mat, src_color, display=False):\n",
    "    transformed_hand_points = {cat:manual_warp_points(hand_dictionary[cat], warp_mat) \n",
    "                              for cat in hand_dictionary}\n",
    "    \n",
    "    \n",
    "    #print(\"in compute transformed points, final_wrp is \\n\", warp_mat)\n",
    "    if display:\n",
    "        warp_dst = perform_warp(src_color,warp_mat, display=False)\n",
    "        plt.imshow(warp_dst, origin='lower')\n",
    "\n",
    "        for cat in transformed_hand_points:\n",
    "            cat_points =  transformed_hand_points[cat]\n",
    "            plt.plot(cat_points[0], cat_points[1], 'r.', alpha=0.5)\n",
    "        plt.title(\"Location hand keypoints after transformation\")\n",
    "\n",
    "        plt.show()\n",
    "    \n",
    "    return transformed_hand_points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Function to Extract transformed hand points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not local:\n",
    "    video_pth = \"/home/intern-2023-kaitlin/data/dataset/burgmuller-performances-from-top/b-01.mp4\"\n",
    "else:\n",
    "    video_pth = \"../data/dataset/testing/b-01.mp4\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Visualization parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vis parameters\n",
    "markersize = 5\n",
    "rh_keypoint_color = np.array([0, 0,255])\n",
    "lh_keypoint_color = np.array([0, 0, 255])\n",
    "\n",
    "rh_cloud_color = np.array([0, 100, 0]).astype(np.uint8)\n",
    "lh_cloud_color = np.array([0, 100,255]).astype(np.uint8)\n",
    "\n",
    "obs_idx = [0,12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saveVid=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_notecloud(warp_dst, transformed_hand_dct, hand_cat, color):\n",
    "    bounds = (np.min(transformed_hand_dct[hand_cat][0, :]), \n",
    "                         np.max(transformed_hand_dct[hand_cat][0, :]))\n",
    "    warp_dst[:,int(bounds[0]): int(bounds[1]), :]+=color\n",
    "    \n",
    "    return warp_dst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_handpoints(warp_dst, transformed_hand_dct,  hand_cat, obs_idx, color):\n",
    "\n",
    "    #Draw keypoints\n",
    "    reduced_landmarks = transformed_hand_dct[hand_cat][:,obs_idx ].T\n",
    "    #print(reduced_landmarks)\n",
    "    \n",
    "    for landmark in reduced_landmarks:\n",
    "        x = int(landmark[0])\n",
    "        y=int(landmark[1])\n",
    "\n",
    "        #print(\"landmark is\", landmark, \"poisiton\", x,y)\n",
    "        warp_dst[(y-markersize):(y+markersize), (x-markersize):(x+markersize), :]=color          \n",
    "\n",
    "    return warp_dst\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Function for extracting info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_hand_info_from_burgmuller(vid_pth, boundary_frame, \n",
    "                                      landmarker_model_pth, final_wrp,\n",
    "                                      saveVid = False, outfile_stem = None, display=False):\n",
    "\n",
    "    '''\n",
    "    Video detector. Defining here for ease of debugging (it keeps an index of current progress)\n",
    "\n",
    "    '''\n",
    "    \n",
    "    #shift image up\n",
    "    warp_mat = final_wrp.copy()\n",
    "    warp_mat[1,2]+= 300\n",
    "    \n",
    "    \n",
    "    base_options = python.BaseOptions(model_asset_path=landmarker_model_pth)\n",
    "    options = vision.HandLandmarkerOptions(base_options=base_options,  \n",
    "                                           running_mode=VisionRunningMode.VIDEO,\n",
    "                                           num_hands=2)\n",
    "    video_detector = vision.HandLandmarker.create_from_options(options)\n",
    "\n",
    "    '''\n",
    "    Input video\n",
    "    '''\n",
    "    cap = cv2.VideoCapture(vid_pth)\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    fpms = fps/1000 #frames per ms\n",
    "    print(\"frames per ms\", fpms)\n",
    "    total_num_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    print(\"total frames in video\", total_num_frames)\n",
    "    ## get width and height\n",
    "    frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "    \n",
    "    \n",
    "    # Loop through each frame in the video using VideoCapture#read()\n",
    "    cur_frame = 0\n",
    "\n",
    "    '''\n",
    "    Output video to see hand bounds\n",
    "    Make first video now\n",
    "    '''\n",
    "    if saveVid:\n",
    "        out1_pth = outfile_stem+\"-1.avi\"\n",
    "        print(\"path of first video file is\",out1_pth)\n",
    "        out1 = cv2.VideoWriter(out1_pth,\n",
    "                            cv2.VideoWriter_fourcc('M','J','P','G'), fps, (frame_width, frame_height))\n",
    "\n",
    "\n",
    "    #Save bounds\n",
    "    lh_res=[]\n",
    "\n",
    "    rh_res =[]\n",
    "\n",
    "    while  cur_frame < total_num_frames:\n",
    "\n",
    "\n",
    "        if cur_frame%500 ==0:\n",
    "            print(\"\\n\\n\", cur_frame, \"out of\", total_num_frames)\n",
    "        cur_frame +=1 \n",
    "\n",
    "        ret, frame = cap.read()\n",
    "        \n",
    "\n",
    "        frame.flags.writeable = False\n",
    "        #adjust orientation\n",
    "        frame= prepare_frame(frame)\n",
    "\n",
    "\n",
    "        frame_timestamp_ms = int(cur_frame/fpms)\n",
    "        #print(ret)\n",
    "\n",
    "        hand_dictionary = get_hand_landmarks(frame, video_detector, \n",
    "                                       timestamp = frame_timestamp_ms,\n",
    "                                      display=False)\n",
    "\n",
    "        transformed_hand_dct= compute_transformed_hand_points(hand_dictionary, \n",
    "                                                             warp_mat, frame, \n",
    "                                                             display=False)\n",
    "\n",
    "\n",
    "        warp_dst = perform_warp(frame,warp_mat, display=False)\n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "        '''\n",
    "        Plot bounding box and save results depending on hand\n",
    "        '''\n",
    "        if 'Right' in transformed_hand_dct:\n",
    "            \n",
    "            #Draw bounds\n",
    "            warp_dst = draw_notecloud(warp_dst, transformed_hand_dct, \"Right\", rh_cloud_color )\n",
    "\n",
    "            #Draw hand points\n",
    "            warp_dst = draw_handpoints(warp_dst, transformed_hand_dct,  \"Right\", obs_idx, rh_keypoint_color)\n",
    "\n",
    "\n",
    "            #Save hand landmark data        \n",
    "            rh_res.append(transformed_hand_dct['Right'])\n",
    "\n",
    "        else:\n",
    "            rh_res.append([])\n",
    "            \n",
    "            \n",
    "        if 'Left' in transformed_hand_dct:\n",
    "            \n",
    "            # Draw bounds\n",
    "            warp_dst = draw_notecloud(warp_dst, transformed_hand_dct, \"Left\", lh_cloud_color )\n",
    "\n",
    "            # Draw handpoints\n",
    "            warp_dst = draw_handpoints(warp_dst, transformed_hand_dct,  \"Left\", obs_idx, lh_keypoint_color)\n",
    "            \n",
    "            \n",
    "            lh_res.append(transformed_hand_dct['Left'])      \n",
    "        else:\n",
    "            lh_res.append([])\n",
    "\n",
    "        if display:\n",
    "            warp_dst = perform_warp(frame,warp_mat, display=False)\n",
    "            fig=plt.figure()\n",
    "\n",
    "            plt.axvspan(rh_bounds[0], rh_bounds[1],  alpha=0.3, color='red')\n",
    "\n",
    "            plt.axvspan(lh_bounds[0], lh_bounds[1],  alpha=0.3, color='blue')\n",
    "            \n",
    "\n",
    "            plt.imshow(warp_dst, origin='lower')        \n",
    "            \n",
    "            break\n",
    "\n",
    "        if saveVid:\n",
    "            #So vis will show up right\n",
    "            warp_dst = cv2.flip(warp_dst, 0)\n",
    "            #plt.imshow(warp_dst)\n",
    "            #plt.show()\n",
    "            #break\n",
    "\n",
    "            #write to first video\n",
    "            if cur_frame < boundary_frame:\n",
    "                out1.write(warp_dst)\n",
    "            \n",
    "            #close first video, open second video, write first frame\n",
    "            elif cur_frame == boundary_frame:\n",
    "                out1.release()\n",
    "                out2_pth = outfile_stem+\"-2.avi\"\n",
    "                out2 = cv2.VideoWriter(out2_pth,\n",
    "                            cv2.VideoWriter_fourcc('M','J','P','G'), fps, (frame_width, frame_height))\n",
    "                out2.write(warp_dst)\n",
    "            else:\n",
    "                out2.write(warp_dst)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    cap.release()\n",
    "    if saveVid:\n",
    "\n",
    "        out2.release()\n",
    "        \n",
    "    return rh_res, lh_res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Extract warp and handpoints from all videos\n",
    "\n",
    "Generates visualization of mediapipe landmarks on video, as well as two files in the form \"_NOTECLOUD_.pkl\" specifiying landmarks on right and left hand. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Read template\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not local:\n",
    "    test_img_file = \"./small_assets/top_piano_piecestart.jpeg\"\n",
    "    template_FILE  = \"./small_assets/template.png\"\n",
    "\n",
    "else:\n",
    "    test_img_file =\"../data/dataset/testing/top_piano_piecestart.jpeg\"\n",
    "    template_FILE  = \"../data/dataset/testing/template.png\"\n",
    "template_color =  cv2.imread(cv2.samples.findFile(template_FILE  ))\n",
    "template_color  = cv2.flip(template_color , 0)\n",
    "T_HEIGHT = template_color.shape[0]\n",
    "T_WIDTH = template_color.shape[1]\n",
    "\n",
    "plt.imshow(template_color)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adjust Hard coded parameters ( position of middle fingers in the first frame)\n",
    "\n",
    "`scorepoints`  - the locations of the middle fingers on the left and right hand on the reference frame. Unit is length of white key. The location of middle C is hardcoded, so these values can be specified relatively. \n",
    "`start` - Location (frames) in the video where reference frame is taken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mf_points = {\n",
    "#'B01':{'vidfile':'b-01.mp4', 'scorepoints': [18.5, 32.5], \"start\":0},\n",
    "#'B02':{'vidfile':'b-02.mp4', 'scorepoints':[MIDDLE_C_POS, MIDDLE_C_POS+7], \"start\":0},#C4, C5\n",
    "#'B03':{'vidfile':'b-03.mp4', 'scorepoints':[MIDDLE_C_POS-1, MIDDLE_C_POS+7], \"start\":0},#C4, C5\n",
    "'B04':{'vidfile':'b-04.mp4', 'scorepoints':[MIDDLE_C_POS-2, MIDDLE_C_POS+8], \"start\":0},#C4, C5    \n",
    "#'B05':{'vidfile':'b-05.mp4', 'scorepoints':[MIDDLE_C_POS-2, MIDDLE_C_POS+12], \"start\":0},#B05: A3 A5\n",
    "#'B06':{'vidfile':'b-06.mp4', 'scorepoints':[MIDDLE_C_POS-5, MIDDLE_C_POS+4], \"start\":0},#B06: E3 G3\n",
    "#'B07':{'vidfile':'b-07.mp4', 'scorepoints':[MIDDLE_C_POS-8.5, MIDDLE_C_POS+1.5], \"start\":0},#B06: E3 G3\n",
    "\n",
    "#'B08':{'vidfile':'b-08.mp4', 'scorepoints':[MIDDLE_C_POS-2, MIDDLE_C_POS+7], \"start\":0},    \n",
    "#'B10':{'vidfile':'b-10.mp4', 'scorepoints':[MIDDLE_C_POS-0, MIDDLE_C_POS+9], \"start\":5},\n",
    "#'B11':{'vidfile':'b-11.mp4', 'scorepoints':[MIDDLE_C_POS+3, MIDDLE_C_POS+10], \"start\":5},\n",
    "#'B13':{'vidfile':'b-13.mp4', 'scorepoints':[MIDDLE_C_POS-1, MIDDLE_C_POS+7.5], \"start\":0},\n",
    "#'B14':{'vidfile':'b-14.mp4', 'scorepoints':[MIDDLE_C_POS-4.5, MIDDLE_C_POS+2.5], \"start\":4},\n",
    "#'B15':{'vidfile':'b-15.mp4', 'scorepoints':[MIDDLE_C_POS-9, MIDDLE_C_POS+1.5], \"start\":4},\n",
    "#'B20':{'vidfile':'b-20.mp4', 'scorepoints':[MIDDLE_C_POS-6, MIDDLE_C_POS+3], \"start\":0},\n",
    "    \n",
    "    # need to run ones below\n",
    "#'B16':{'vidfile':'b-16.mp4', 'scorepoints':[MIDDLE_C_POS+0.5, MIDDLE_C_POS+8.5], \"start\":8},\n",
    "\n",
    "#'B21':{'vidfile':'b-21.mp4', 'scorepoints':[MIDDLE_C_POS-.5, MIDDLE_C_POS+6], \"start\":0},\n",
    "'B24':{'vidfile':'b-24.mp4', 'scorepoints':[MIDDLE_C_POS-7, MIDDLE_C_POS+4], \"start\":12},\n",
    "#try different parse! 'B16':{'vidfile':'b-16.mp4', 'scorepoints':[MIDDLE_C_POS-1.5, MIDDLE_C_POS+6], \"start\":0},\n",
    " #try different parse! 'B19':{'vidfile':'b-19.mp4', 'scorepoints':[MIDDLE_C_POS-0.5, MIDDLE_C_POS+7.5], \"start\":0},\n",
    " #try different parse!'B20':{'vidfile':'b-20.mp4', 'scorepoints':[MIDDLE_C_POS-6, MIDDLE_C_POS+3.5], \"start\":4},\n",
    "}\n",
    "\n",
    "#B06: E3 G3\n",
    "#B07: A#2 D#4\n",
    "#B08: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Run fun pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display = True\n",
    "home_dir = \"../data/dataset/Burgmuller_top_kaitlin\"\n",
    "saveVid = True\n",
    "for folder in mf_points:\n",
    "\n",
    "\n",
    "        print(\"*******************\\nFile\", folder)\n",
    "\n",
    "        vid_pth= os.path.join(home_dir, folder, mf_points[folder]['vidfile'])\n",
    "        \n",
    "        div_pth = os.path.join(home_dir, folder, 'div.txt')\n",
    "        with open(div_pth) as f:\n",
    "            contents = f.readlines()\n",
    "            boundary_time = int(contents[0])\n",
    "                  \n",
    "\n",
    "        #get alignment frame\n",
    "        refHandTime = mf_points[folder][\"start\"]\n",
    "        hand_ref_frame, alignment_computation_frame, boundary_frame, fps= get_boundary_frame(\n",
    "            vid_pth, boundary_time,refHandTime)\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "        scorepoints = mf_points[folder]['scorepoints']\n",
    "        final_wrp = get_piano_alignment_transform(template_color=template_color, \n",
    "                                                  src_color=hand_ref_frame, \n",
    "                                                  alignment_computation_frame=alignment_computation_frame,\n",
    "                                                  detector=detector, \n",
    "                                                  middle_finger_notes=scorepoints, \n",
    "                                                  display=True)\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        initialized_prev_warp = final_wrp.copy()\n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "        outfile_stem = os.path.join(home_dir, folder, \"NOTECLOUD_\"+folder)\n",
    "        rh_res, lh_res = extract_hand_info_from_burgmuller(vid_pth, boundary_frame, \n",
    "                                          landmarker_model_pth, final_wrp,\n",
    "                                          saveVid=saveVid, outfile_stem=outfile_stem)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        rh_res_pth =  os.path.join(home_dir, folder,\"NOTECLOUD_\"+folder+\"-rh.pkl\")\n",
    "        lh_res_pth =  os.path.join(home_dir, folder,\"NOTECLOUD_\"+folder+\"-lh.pkl\")\n",
    "        print(\"saved paths are\", rh_res_pth, lh_res_pth)\n",
    "        \n",
    "        with open(rh_res_pth  , 'wb') as handle:\n",
    "            pickle.dump({'boundary': boundary_frame, 'fps': fps, 'data':rh_res}, handle, protocol=pickle.HIGHEST_PROTOCOL) \n",
    "        \n",
    "        \n",
    "        with open(lh_res_pth  , 'wb') as handle:\n",
    "            pickle.dump({'boundary': boundary_frame, 'fps': fps,'data':lh_res}, handle, protocol=pickle.HIGHEST_PROTOCOL)    \n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Version where previously computed initialization is used (for takes with issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mf_points = {\n",
    "#'B01':{'vidfile':'b-01.mp4', 'scorepoints': [18.5, 32.5], \"start\":0},\n",
    "#'B02':{'vidfile':'b-02.mp4', 'scorepoints':[MIDDLE_C_POS, MIDDLE_C_POS+7], \"start\":0},#C4, C5\n",
    "#'B03':{'vidfile':'b-03.mp4', 'scorepoints':[MIDDLE_C_POS-1, MIDDLE_C_POS+7], \"start\":0},#C4, C5\n",
    "#'B04':{'vidfile':'b-04.mp4', 'scorepoints':[MIDDLE_C_POS-2, MIDDLE_C_POS+8], \"start\":0},#C4, C5    \n",
    "#'B05':{'vidfile':'b-05.mp4', 'scorepoints':[MIDDLE_C_POS-2, MIDDLE_C_POS+12], \"start\":0},#B05: A3 A5\n",
    "#'B06':{'vidfile':'b-06.mp4', 'scorepoints':[MIDDLE_C_POS-5, MIDDLE_C_POS+4], \"start\":0},#B06: E3 G3\n",
    "#'B07':{'vidfile':'b-07.mp4', 'scorepoints':[MIDDLE_C_POS-8.5, MIDDLE_C_POS+1.5], \"start\":0},#B06: E3 G3\n",
    "\n",
    "#'B08':{'vidfile':'b-08.mp4', 'scorepoints':[MIDDLE_C_POS-2, MIDDLE_C_POS+7], \"start\":0},    \n",
    "#'B10':{'vidfile':'b-10.mp4', 'scorepoints':[MIDDLE_C_POS-0, MIDDLE_C_POS+9], \"start\":5},\n",
    "#'B11':{'vidfile':'b-11.mp4', 'scorepoints':[MIDDLE_C_POS+3, MIDDLE_C_POS+10], \"start\":5},\n",
    "#'B13':{'vidfile':'b-13.mp4', 'scorepoints':[MIDDLE_C_POS-1, MIDDLE_C_POS+7.5], \"start\":0},\n",
    "#'B14':{'vidfile':'b-14.mp4', 'scorepoints':[MIDDLE_C_POS-4.5, MIDDLE_C_POS+2.5], \"start\":4},\n",
    "#'B15':{'vidfile':'b-15.mp4', 'scorepoints':[MIDDLE_C_POS-9, MIDDLE_C_POS+1.5], \"start\":4},\n",
    "#'B20':{'vidfile':'b-20.mp4', 'scorepoints':[MIDDLE_C_POS-6, MIDDLE_C_POS+3], \"start\":0},\n",
    "#'B24':{'vidfile':'b-24.mp4', 'scorepoints':[MIDDLE_C_POS-7, MIDDLE_C_POS+4], \"start\":12},    \n",
    "    # need to run ones below\n",
    "#'B16':{'vidfile':'b-16.mp4', 'scorepoints':[MIDDLE_C_POS+0.5, MIDDLE_C_POS+8.5], \"start\":8},\n",
    "\n",
    "'B21':{'vidfile':'b-21.mp4', 'scorepoints':[MIDDLE_C_POS-.5, MIDDLE_C_POS+6], \"start\":0},\n",
    "\n",
    "#try different parse! 'B16':{'vidfile':'b-16.mp4', 'scorepoints':[MIDDLE_C_POS-1.5, MIDDLE_C_POS+6], \"start\":0},\n",
    "'B19':{'vidfile':'b-19.mp4', 'scorepoints':[MIDDLE_C_POS-0.5, MIDDLE_C_POS+7.5], \"start\":0},\n",
    "'B20':{'vidfile':'b-20.mp4', 'scorepoints':[MIDDLE_C_POS-6, MIDDLE_C_POS+3.5], \"start\":4},\n",
    "}\n",
    "\n",
    "#B06: E3 G3\n",
    "#B07: A#2 D#4\n",
    "#B08: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display = True\n",
    "home_dir = \"../data/dataset/Burgmuller_top_kaitlin\"\n",
    "saveVid = True\n",
    "for folder in mf_points:\n",
    "\n",
    "\n",
    "        print(\"*******************\\nFile\", folder)\n",
    "\n",
    "        vid_pth= os.path.join(home_dir, folder, mf_points[folder]['vidfile'])\n",
    "        \n",
    "        div_pth = os.path.join(home_dir, folder, 'div.txt')\n",
    "        with open(div_pth) as f:\n",
    "            contents = f.readlines()\n",
    "            boundary_time = int(contents[0])\n",
    "                  \n",
    "\n",
    "        #get alignment frame\n",
    "        refHandTime = mf_points[folder][\"start\"]\n",
    "        hand_ref_frame, alignment_computation_frame, boundary_frame, fps= get_boundary_frame(\n",
    "            vid_pth, boundary_time,refHandTime)\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "        scorepoints = mf_points[folder]['scorepoints']\n",
    "        '''\n",
    "        final_wrp = get_piano_alignment_transform(template_color=template_color, \n",
    "                                                  src_color=hand_ref_frame, \n",
    "                                                  alignment_computation_frame=alignment_computation_frame,\n",
    "                                                  detector=detector, \n",
    "                                                  middle_finger_notes=scorepoints, \n",
    "                                                  display=True, prevComputedInitialization=initialized_prev_warp)\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        '''        \n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "        outfile_stem = os.path.join(home_dir, folder, \"NOTECLOUD_\"+folder)\n",
    "        rh_res, lh_res = extract_hand_info_from_burgmuller(vid_pth, boundary_frame, \n",
    "                                          landmarker_model_pth, final_wrp,\n",
    "                                          saveVid=saveVid, outfile_stem=outfile_stem)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        rh_res_pth =  os.path.join(home_dir, folder,\"NOTECLOUD_\"+folder+\"-rh.pkl\")\n",
    "        lh_res_pth =  os.path.join(home_dir, folder,\"NOTECLOUD_\"+folder+\"-lh.pkl\")\n",
    "        print(\"saved paths are\", rh_res_pth, lh_res_pth)\n",
    "        \n",
    "        with open(rh_res_pth  , 'wb') as handle:\n",
    "            pickle.dump({'boundary': boundary_frame, 'fps': fps, 'data':rh_res}, handle, protocol=pickle.HIGHEST_PROTOCOL) \n",
    "        \n",
    "        \n",
    "        with open(lh_res_pth  , 'wb') as handle:\n",
    "            pickle.dump({'boundary': boundary_frame, 'fps': fps,'data':lh_res}, handle, protocol=pickle.HIGHEST_PROTOCOL)    \n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Tutorial: Extract single video info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constants\n",
    "\n",
    "- path to video\n",
    "- path to `div.txt` file specifying time (in seconds) that is boundary between take 1 and take 2 of the piece. This file needs to be manually created by looking at the video\n",
    "- refHandTime: location (in frames) of the frame used for alignment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vid_pth = '../data/dataset/Burgmuller_top_kaitlin/B02/b-02.mp4'\n",
    "div_pth = '../data/dataset/Burgmuller_top_kaitlin/B02/div.txt'\n",
    "refHandTime = 0 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract and visualize alignment frame\n",
    "\n",
    "Visually check that boundary frame is empty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract boundary\n",
    "with open(div_pth) as f:\n",
    "    contents = f.readlines()\n",
    "    boundary_time = int(contents[0])\n",
    "                  \n",
    "\n",
    "#get alignment frame\n",
    "refHandTime = mf_points[folder][\"start\"]\n",
    "hand_ref_frame, alignment_computation_frame, boundary_frame, fps= get_boundary_frame(\n",
    "    vid_pth, boundary_time,refHandTime)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visually inspect the alginment frame for the positions of the two middle fingers in relation to middle C. Specify their location in terms of white notes away from Middle C\n",
    "\n",
    "- New data has fixed reference position, but for this dataset finger locations must be manually inputted. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scorepoints = [MIDDLE_C_POS, MIDDLE_C_POS+7]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtain warp matrix needed to shift alignment frame to predetermined template location\n",
    "\n",
    "As a sanity check, make sure the notes specified by the red corners in the image match those in the template and that the mediapipe landmarks looks generally correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "final_wrp = get_piano_alignment_transform(template_color=template_color, \n",
    "                                          src_color=hand_ref_frame, \n",
    "                                          alignment_computation_frame=alignment_computation_frame,\n",
    "                                          detector=detector, \n",
    "                                          middle_finger_notes=scorepoints, \n",
    "                                          display=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract hand landmark points. This is the raw data, postprocessing is performed in `FINAL PIPELINE 3`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#path for videos\n",
    "outfile_stem = os.path.join(home_dir, folder, \"NOTECLOUD_\"+folder)\n",
    "\n",
    "#path for extracted landmarks\n",
    "rh_res_pth =  os.path.join(home_dir, folder,\"NOTECLOUD_\"+folder+\"-rh.pkl\")\n",
    "lh_res_pth =  os.path.join(home_dir, folder,\"NOTECLOUD_\"+folder+\"-lh.pkl\")\n",
    "print(\"saved paths are\", rh_res_pth, lh_res_pth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rh_res, lh_res = extract_hand_info_from_burgmuller(vid_pth, boundary_frame, \n",
    "                                  landmarker_model_pth, final_wrp,\n",
    "                                  saveVid=saveVid, outfile_stem=outfile_stem)\n",
    "\n",
    "with open(rh_res_pth  , 'wb') as handle:\n",
    "    pickle.dump({'boundary': boundary_frame, 'fps': fps, 'data':rh_res}, handle, protocol=pickle.HIGHEST_PROTOCOL) \n",
    "\n",
    "\n",
    "with open(lh_res_pth  , 'wb') as handle:\n",
    "    pickle.dump({'boundary': boundary_frame, 'fps': fps,'data':lh_res}, handle, protocol=pickle.HIGHEST_PROTOCOL)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
